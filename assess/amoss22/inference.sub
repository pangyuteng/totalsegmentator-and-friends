universe = docker
#docker_image = wasserth/totalsegmentator_container:master
docker_image = pangyuteng/totalsegmentator:latest

executable = inference.sh
transfer_input_files = inference.sh
should_transfer_files = YES
when_to_transfer_output = ON_EXIT

log = log/inference.$(cluster).$(process).log
error = log/inference.$(cluster).$(process).err
output = log/inference.$(cluster).$(process).out

#requirements = (OpSys == "LINUX" && Arch == "X86_64" && GPUMEM > 20000 )
# ^^^ a few not running, trying out gpu with >20GB RAM

requirements = (OpSys == "LINUX" && Arch == "X86_64" && GPUMEM > 10000 )
request_cpus = 4
request_gpus = 1
request_memory = 20GB
request_disk = 20GB

max_materialize = 5
arguments = "$(myargs)"
queue myargs from inference.args

