universe = docker
docker_image = wasserth/totalsegmentator_container:master

executable = inference.sh
transfer_input_files = inference.sh
should_transfer_files = YES
when_to_transfer_output = ON_EXIT

log = log/inference.$(cluster).$(process).log
error = log/inference.$(cluster).$(process).err
output = log/inference.$(cluster).$(process).out

# # first pass
# requirements = (OpSys == "LINUX" && Arch == "X86_64" && GPUMEM < 12000 )
# a few not running, trying out gpu with >20GB RAM
requirements = (OpSys == "LINUX" && Arch == "X86_64" && GPUMEM > 20000 )
request_cpus = 1
request_gpus = 1
request_memory = 20GB
request_disk = 20GB

max_materialize = 5
arguments = "$(myargs)"
queue myargs from inference.args

